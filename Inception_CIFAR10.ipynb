{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "af1745a9019c4613ac5f5c9a2f2dcc83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4f780a8242cb48ddb63daf2eb0352aba",
              "IPY_MODEL_79158446b32543d686a8b7249a55e4b8",
              "IPY_MODEL_dce4e628bb5a40b0befabcf201d27ea4"
            ],
            "layout": "IPY_MODEL_92a632f6ad0f40419120837d94cc762d"
          }
        },
        "4f780a8242cb48ddb63daf2eb0352aba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_11340795b2cf4cfb82548cf02a8951fd",
            "placeholder": "​",
            "style": "IPY_MODEL_1e53671bff8247e8b224ba0376943a3a",
            "value": " 13%"
          }
        },
        "79158446b32543d686a8b7249a55e4b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40f893e3d088455fb3e37d5a0cfe70d7",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a8b0970397de4d22b198d2ba65aad484",
            "value": 13
          }
        },
        "dce4e628bb5a40b0befabcf201d27ea4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b968c19905eb489a83ca22281ed530c5",
            "placeholder": "​",
            "style": "IPY_MODEL_e405040a1e73422b9f575a50ec1cf7aa",
            "value": " 13/100 [18:09&lt;1:54:48, 79.18s/it]"
          }
        },
        "92a632f6ad0f40419120837d94cc762d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11340795b2cf4cfb82548cf02a8951fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e53671bff8247e8b224ba0376943a3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "40f893e3d088455fb3e37d5a0cfe70d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8b0970397de4d22b198d2ba65aad484": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b968c19905eb489a83ca22281ed530c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e405040a1e73422b9f575a50ec1cf7aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hvzHkWepmpDY"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    from torchinfo import summary\n",
        "except:\n",
        "    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
        "    !pip install -q torchinfo\n",
        "    from torchinfo import summary"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "JFAWd3BnmvH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "FOtsBqqXmvei",
        "outputId": "9a97abcb-fa4f-4fdc-f08b-6bc247863a70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 284
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(p=0.1),\n",
        "    transforms.RandomVerticalFlip(p=0.1),\n",
        "    transforms.RandomRotation(degrees=(0, 90)),\n",
        "    #transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
        "    ToTensor()\n",
        "])"
      ],
      "metadata": {
        "id": "g2-HrOuSmwtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "train_data = torchvision.datasets.CIFAR10(\n",
        "    root='CIFAR10/train',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=train_transform,\n",
        "    #transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_data = torchvision.datasets.CIFAR10(\n",
        "    root='CIFAR10/test',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehfqBoZgm0PS",
        "outputId": "7857da0b-f8d2-4c50-d150-733a0a66f4bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHrJ--D9m1nq",
        "outputId": "7afa189a-0563-4689-f0f7-5d021937893d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Dataset CIFAR10\n",
              "     Number of datapoints: 50000\n",
              "     Root location: CIFAR10/train\n",
              "     Split: Train\n",
              "     StandardTransform\n",
              " Transform: Compose(\n",
              "                RandomHorizontalFlip(p=0.1)\n",
              "                RandomVerticalFlip(p=0.1)\n",
              "                RandomRotation(degrees=[0.0, 90.0], interpolation=nearest, expand=False, fill=0)\n",
              "                ToTensor()\n",
              "            ),\n",
              " Dataset CIFAR10\n",
              "     Number of datapoints: 10000\n",
              "     Root location: CIFAR10/test\n",
              "     Split: Test\n",
              "     StandardTransform\n",
              " Transform: ToTensor())"
            ]
          },
          "metadata": {},
          "execution_count": 287
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.classes, len(train_data.classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prsVnnYem2u7",
        "outputId": "0b561de6-3950-4322-ff6e-b659b2992290"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['airplane',\n",
              "  'automobile',\n",
              "  'bird',\n",
              "  'cat',\n",
              "  'deer',\n",
              "  'dog',\n",
              "  'frog',\n",
              "  'horse',\n",
              "  'ship',\n",
              "  'truck'],\n",
              " 10)"
            ]
          },
          "metadata": {},
          "execution_count": 288
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image, label = train_data[0]\n",
        "\n",
        "image = image.permute(1, 2, 0)\n",
        "plt.imshow(image.squeeze())\n",
        "plt.title(label)\n",
        "plt.axis(\"off\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "KMl7Uno6m3_c",
        "outputId": "3321345d-bd8b-4cd8-9815-756751b72863"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-0.5, 31.5, 31.5, -0.5)"
            ]
          },
          "metadata": {},
          "execution_count": 289
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    dataset=train_data,\n",
        "    batch_size=64,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    dataset=test_data,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "train_loader, test_loader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUA3qpEAm5T0",
        "outputId": "30185d8f-1170-4282-dae3-a58b51469cfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<torch.utils.data.dataloader.DataLoader at 0x787acb9f5f00>,\n",
              " <torch.utils.data.dataloader.DataLoader at 0x787acb9f6a70>)"
            ]
          },
          "metadata": {},
          "execution_count": 290
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_loader), len(test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Len-KjL4m86i",
        "outputId": "c698bc6e-8b6f-402e-9833-cd22de5bca52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(782, 10000)"
            ]
          },
          "metadata": {},
          "execution_count": 291
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "examples = enumerate(train_loader)\n",
        "batch_idx, (example_data, example_targets) = next(examples)"
      ],
      "metadata": {
        "id": "tAHPdJzMm-Gz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inception V1.5 Block\n",
        "class InceptionV1_block(nn.Module):\n",
        "    def __init__(self, in_channels, out_1x1, red_3x3, out_3x3, red_5x5, out_5x5, out_1x1pool, dimension_reductions: bool=True):\n",
        "        super().__init__()\n",
        "        self.branch_1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=in_channels, out_channels=out_1x1, kernel_size=1),\n",
        "            nn.BatchNorm2d(out_1x1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.branch_2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=in_channels, out_channels=red_3x3, kernel_size=1),\n",
        "            nn.BatchNorm2d(red_3x3),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=red_3x3, out_channels=out_3x3, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_3x3),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.branch_3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=in_channels, out_channels=red_5x5, kernel_size=1),\n",
        "            nn.BatchNorm2d(red_5x5),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=red_5x5, out_channels=out_5x5, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(out_5x5),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.branch_4 = nn.Sequential(\n",
        "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
        "            nn.Conv2d(in_channels=in_channels, out_channels=out_1x1pool, kernel_size=1),\n",
        "            nn.BatchNorm2d(out_1x1pool),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        first_block_out = self.branch_1(x)\n",
        "        second_block_out = self.branch_2(x)\n",
        "        third_block_out = self.branch_3(x)\n",
        "        fourth_block_out = self.branch_4(x)\n",
        "\n",
        "        return torch.concat([first_block_out, second_block_out, third_block_out, fourth_block_out], dim=1)"
      ],
      "metadata": {
        "id": "YqXxbdYgnF7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inception V2 Block\n",
        "class InceptionV2_block(nn.Module):\n",
        "    def __init__(self, in_channels, out_1x1, red_3x3, out_3x3, red_5x5, out_5x5, out_1x1pool, dimension_reductions: bool=True):\n",
        "        super().__init__()\n",
        "        self.branch_1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=in_channels, out_channels=out_1x1, kernel_size=1),\n",
        "            nn.BatchNorm2d(out_1x1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.branch_2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=in_channels, out_channels=red_3x3, kernel_size=1),\n",
        "            nn.BatchNorm2d(red_3x3),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=red_3x3, out_channels=out_3x3, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_3x3),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.branch_3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=in_channels, out_channels=red_5x5, kernel_size=1),\n",
        "            nn.BatchNorm2d(red_5x5),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=red_5x5, out_channels=out_5x5, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_5x5),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=out_5x5, out_channels=out_5x5, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_5x5),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.branch_4 = nn.Sequential(\n",
        "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
        "            nn.Conv2d(in_channels=in_channels, out_channels=out_1x1pool, kernel_size=1),\n",
        "            nn.BatchNorm2d(out_1x1pool),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        first_block_out = self.branch_1(x)\n",
        "        second_block_out = self.branch_2(x)\n",
        "        third_block_out = self.branch_3(x)\n",
        "        fourth_block_out = self.branch_4(x)\n",
        "\n",
        "        return torch.concat([first_block_out, second_block_out, third_block_out, fourth_block_out], dim=1)"
      ],
      "metadata": {
        "id": "KUUmXDMcnIRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Resnet Block\n",
        "class ResNet_convert_dim(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride):\n",
        "        super().__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=stride),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)"
      ],
      "metadata": {
        "id": "R7Eip5gTnqyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inception V2\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__() # 3 x 32 x 32\n",
        "        self.block_1 = nn.Sequential(\n",
        "            #nn.Conv2d(in_channels=3, out_channels=32, kernel_size=5, padding=2), # 32 x 32 x 32\n",
        "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1), # 32 x 32 x 32\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1), # 64 x 32 x 32\n",
        "            #nn.Dropout(0.35),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2), # 64 x 16 x 16\n",
        "        )\n",
        "        self.res_1 = ResNet_convert_dim(3, 64, 2)\n",
        "        self.block_2 = nn.Sequential(\n",
        "            #nn.Conv2d(in_channels=32, out_channels=128, kernel_size=5, padding=2), # 128 x 16 x 16\n",
        "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1), # 128 x 16 x 16\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1), # 256 x 16 x 16\n",
        "            #nn.Dropout(0.35),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2) # 256 x 8 x 8\n",
        "        )\n",
        "        self.res_2 = ResNet_convert_dim(64, 256, 2) # 128 x 8 x 8\n",
        "        self.block_3 = nn.Sequential(\n",
        "            InceptionV2_block(in_channels=256, out_1x1=200, red_3x3=96, out_3x3=100, red_5x5=64, out_5x5=50, out_1x1pool=50), # 400 x 8 x 8\n",
        "            #nn.Dropout(0.35),\n",
        "            #nn.MaxPool2d(kernel_size=2), # 240 x 4 x 4\n",
        "            InceptionV2_block(in_channels=240, out_1x1=256, red_3x3=96, out_3x3=128, red_5x5=64, out_5x5=64, out_1x1pool=32), # 480 x 8 x 8\n",
        "            nn.MaxPool2d(kernel_size=2) # 480 x 4 x 4\n",
        "        )\n",
        "        self.res_3 = ResNet_convert_dim(128, 960, 4)\n",
        "        self.block_4 = nn.Sequential( # 240 x 4 x 4\n",
        "            InceptionV2_block(in_channels=480, out_1x1=512, red_3x3=96, out_3x3=256, red_5x5=64, out_5x5=128, out_1x1pool=64), # 960 x 4 x 4\n",
        "            #nn.Dropout(0.35),\n",
        "            nn.MaxPool2d(kernel_size=2), # 960 x 2 x 2\n",
        "            #InceptionV2_block(in_channels=4)\n",
        "        )\n",
        "        #self.res_3 = ResNet_convert_dim(128, 960, 4) # 480 x 2 x 2\n",
        "        self.block_5 = nn.Sequential(\n",
        "            InceptionV2_block(in_channels=960, out_1x1=512, red_3x3=96, out_3x3=256, red_5x5=64, out_5x5=128, out_1x1pool=64), # 960 x 2 x 2\n",
        "            nn.Dropout(0.35),\n",
        "            nn.MaxPool2d(kernel_size=2) # 960 x 1 x 1\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(), # 960 x 1 x 1\n",
        "            nn.Linear(in_features=960*2*2, out_features=1024),\n",
        "            nn.Dropout(0.35),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=1024, out_features=256),\n",
        "            nn.Linear(in_features=512, out_features=10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor): # 3 x 32 x 32\n",
        "        x_1 = self.block_1(x) + self.res_1(x) # 32 x 16 x 16\n",
        "        x_2 = self.block_2(x_1) + self.res_2(x_1) # 128 x 8 x 8\n",
        "        x_3 = self.block_3(x_2) # 240 x 4 x 4\n",
        "        x_4 = self.block_4(x_3) + self.res_3(x_2) # 960 x 2 x 2\n",
        "        #x_5 = self.block_5(x_4)\n",
        "        x_5 = self.classifier(x_4)\n",
        "        return x_5"
      ],
      "metadata": {
        "id": "Srik2vXanIxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "network = CNN().to(device)\n",
        "network"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9nBbWEhnyVB",
        "outputId": "49773dae-a687-4599-eb54-f2b2a2d0d15b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNN(\n",
              "  (block_1): Sequential(\n",
              "    (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (4): Dropout(p=0.35, inplace=False)\n",
              "    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (6): ReLU()\n",
              "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (res_1): ResNet_convert_dim(\n",
              "    (block): Sequential(\n",
              "      (0): Conv2d(3, 32, kernel_size=(1, 1), stride=(2, 2))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "  )\n",
              "  (block_2): Sequential(\n",
              "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (4): Dropout(p=0.35, inplace=False)\n",
              "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (6): ReLU()\n",
              "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (res_2): ResNet_convert_dim(\n",
              "    (block): Sequential(\n",
              "      (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(2, 2))\n",
              "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "  )\n",
              "  (block_3): Sequential(\n",
              "    (0): InceptionV2_block(\n",
              "      (branch_1): Sequential(\n",
              "        (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU()\n",
              "      )\n",
              "      (branch_2): Sequential(\n",
              "        (0): Conv2d(128, 96, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU()\n",
              "        (3): Conv2d(96, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (5): ReLU()\n",
              "      )\n",
              "      (branch_3): Sequential(\n",
              "        (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU()\n",
              "        (3): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (5): ReLU()\n",
              "        (6): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (8): ReLU()\n",
              "      )\n",
              "      (branch_4): Sequential(\n",
              "        (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
              "        (1): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (3): ReLU()\n",
              "      )\n",
              "    )\n",
              "    (1): Dropout(p=0.35, inplace=False)\n",
              "    (2): InceptionV2_block(\n",
              "      (branch_1): Sequential(\n",
              "        (0): Conv2d(240, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU()\n",
              "      )\n",
              "      (branch_2): Sequential(\n",
              "        (0): Conv2d(240, 96, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU()\n",
              "        (3): Conv2d(96, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (5): ReLU()\n",
              "      )\n",
              "      (branch_3): Sequential(\n",
              "        (0): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU()\n",
              "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (5): ReLU()\n",
              "        (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (8): ReLU()\n",
              "      )\n",
              "      (branch_4): Sequential(\n",
              "        (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
              "        (1): Conv2d(240, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (3): ReLU()\n",
              "      )\n",
              "    )\n",
              "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (res_3): ResNet_convert_dim(\n",
              "    (block): Sequential(\n",
              "      (0): Conv2d(128, 960, kernel_size=(1, 1), stride=(4, 4))\n",
              "      (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "  )\n",
              "  (block_4): Sequential(\n",
              "    (0): InceptionV2_block(\n",
              "      (branch_1): Sequential(\n",
              "        (0): Conv2d(480, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU()\n",
              "      )\n",
              "      (branch_2): Sequential(\n",
              "        (0): Conv2d(480, 96, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU()\n",
              "        (3): Conv2d(96, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (5): ReLU()\n",
              "      )\n",
              "      (branch_3): Sequential(\n",
              "        (0): Conv2d(480, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU()\n",
              "        (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (5): ReLU()\n",
              "        (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (8): ReLU()\n",
              "      )\n",
              "      (branch_4): Sequential(\n",
              "        (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
              "        (1): Conv2d(480, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (3): ReLU()\n",
              "      )\n",
              "    )\n",
              "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (block_5): Sequential(\n",
              "    (0): InceptionV2_block(\n",
              "      (branch_1): Sequential(\n",
              "        (0): Conv2d(960, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU()\n",
              "      )\n",
              "      (branch_2): Sequential(\n",
              "        (0): Conv2d(960, 96, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU()\n",
              "        (3): Conv2d(96, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (5): ReLU()\n",
              "      )\n",
              "      (branch_3): Sequential(\n",
              "        (0): Conv2d(960, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU()\n",
              "        (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (5): ReLU()\n",
              "        (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (8): ReLU()\n",
              "      )\n",
              "      (branch_4): Sequential(\n",
              "        (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
              "        (1): Conv2d(960, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (3): ReLU()\n",
              "      )\n",
              "    )\n",
              "    (1): Dropout(p=0.35, inplace=False)\n",
              "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Flatten(start_dim=1, end_dim=-1)\n",
              "    (1): Linear(in_features=3840, out_features=1024, bias=True)\n",
              "    (2): Dropout(p=0.35, inplace=False)\n",
              "    (3): ReLU()\n",
              "    (4): Linear(in_features=1024, out_features=256, bias=True)\n",
              "    (5): Linear(in_features=256, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 297
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random_input_image = (1, 3, 32, 32)\n",
        "random_input_image_error = (1, 3, 250, 250)\n",
        "\n",
        "summary(CNN(),\n",
        "        input_size=random_input_image,\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eb4nk0MInyxK",
        "outputId": "5598ddf5-482e-48e5-bdd4-13f2a0504149"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "========================================================================================================================\n",
              "Layer (type (var_name))                  Input Shape          Output Shape         Param #              Trainable\n",
              "========================================================================================================================\n",
              "CNN (CNN)                                [1, 3, 32, 32]       [1, 10]              1,152,672            True\n",
              "├─Sequential (block_1)                   [1, 3, 32, 32]       [1, 32, 16, 16]      --                   True\n",
              "│    └─Conv2d (0)                        [1, 3, 32, 32]       [1, 3, 32, 32]       84                   True\n",
              "│    └─BatchNorm2d (1)                   [1, 3, 32, 32]       [1, 3, 32, 32]       6                    True\n",
              "│    └─ReLU (2)                          [1, 3, 32, 32]       [1, 3, 32, 32]       --                   --\n",
              "│    └─Conv2d (3)                        [1, 3, 32, 32]       [1, 32, 32, 32]      896                  True\n",
              "│    └─Dropout (4)                       [1, 32, 32, 32]      [1, 32, 32, 32]      --                   --\n",
              "│    └─BatchNorm2d (5)                   [1, 32, 32, 32]      [1, 32, 32, 32]      64                   True\n",
              "│    └─ReLU (6)                          [1, 32, 32, 32]      [1, 32, 32, 32]      --                   --\n",
              "│    └─MaxPool2d (7)                     [1, 32, 32, 32]      [1, 32, 16, 16]      --                   --\n",
              "├─ResNet_convert_dim (res_1)             [1, 3, 32, 32]       [1, 32, 16, 16]      --                   True\n",
              "│    └─Sequential (block)                [1, 3, 32, 32]       [1, 32, 16, 16]      --                   True\n",
              "│    │    └─Conv2d (0)                   [1, 3, 32, 32]       [1, 32, 16, 16]      128                  True\n",
              "│    │    └─BatchNorm2d (1)              [1, 32, 16, 16]      [1, 32, 16, 16]      64                   True\n",
              "│    │    └─ReLU (2)                     [1, 32, 16, 16]      [1, 32, 16, 16]      --                   --\n",
              "├─Sequential (block_2)                   [1, 32, 16, 16]      [1, 128, 8, 8]       --                   True\n",
              "│    └─Conv2d (0)                        [1, 32, 16, 16]      [1, 64, 16, 16]      18,496               True\n",
              "│    └─BatchNorm2d (1)                   [1, 64, 16, 16]      [1, 64, 16, 16]      128                  True\n",
              "│    └─ReLU (2)                          [1, 64, 16, 16]      [1, 64, 16, 16]      --                   --\n",
              "│    └─Conv2d (3)                        [1, 64, 16, 16]      [1, 128, 16, 16]     73,856               True\n",
              "│    └─Dropout (4)                       [1, 128, 16, 16]     [1, 128, 16, 16]     --                   --\n",
              "│    └─BatchNorm2d (5)                   [1, 128, 16, 16]     [1, 128, 16, 16]     256                  True\n",
              "│    └─ReLU (6)                          [1, 128, 16, 16]     [1, 128, 16, 16]     --                   --\n",
              "│    └─MaxPool2d (7)                     [1, 128, 16, 16]     [1, 128, 8, 8]       --                   --\n",
              "├─ResNet_convert_dim (res_2)             [1, 32, 16, 16]      [1, 128, 8, 8]       --                   True\n",
              "│    └─Sequential (block)                [1, 32, 16, 16]      [1, 128, 8, 8]       --                   True\n",
              "│    │    └─Conv2d (0)                   [1, 32, 16, 16]      [1, 128, 8, 8]       4,224                True\n",
              "│    │    └─BatchNorm2d (1)              [1, 128, 8, 8]       [1, 128, 8, 8]       256                  True\n",
              "│    │    └─ReLU (2)                     [1, 128, 8, 8]       [1, 128, 8, 8]       --                   --\n",
              "├─Sequential (block_3)                   [1, 128, 8, 8]       [1, 480, 4, 4]       --                   True\n",
              "│    └─InceptionV2_block (0)             [1, 128, 8, 8]       [1, 240, 8, 8]       --                   True\n",
              "│    │    └─Sequential (branch_1)        [1, 128, 8, 8]       [1, 128, 8, 8]       16,768               True\n",
              "│    │    └─Sequential (branch_2)        [1, 128, 8, 8]       [1, 64, 8, 8]        68,064               True\n",
              "│    │    └─Sequential (branch_3)        [1, 128, 8, 8]       [1, 32, 8, 8]        36,224               True\n",
              "│    │    └─Sequential (branch_4)        [1, 128, 8, 8]       [1, 16, 8, 8]        2,096                True\n",
              "│    └─Dropout (1)                       [1, 240, 8, 8]       [1, 240, 8, 8]       --                   --\n",
              "│    └─InceptionV2_block (2)             [1, 240, 8, 8]       [1, 480, 8, 8]       --                   True\n",
              "│    │    └─Sequential (branch_1)        [1, 240, 8, 8]       [1, 256, 8, 8]       62,208               True\n",
              "│    │    └─Sequential (branch_2)        [1, 240, 8, 8]       [1, 128, 8, 8]       134,304              True\n",
              "│    │    └─Sequential (branch_3)        [1, 240, 8, 8]       [1, 64, 8, 8]        89,664               True\n",
              "│    │    └─Sequential (branch_4)        [1, 240, 8, 8]       [1, 32, 8, 8]        7,776                True\n",
              "│    └─MaxPool2d (3)                     [1, 480, 8, 8]       [1, 480, 4, 4]       --                   --\n",
              "├─Sequential (block_4)                   [1, 480, 4, 4]       [1, 960, 2, 2]       --                   True\n",
              "│    └─InceptionV2_block (0)             [1, 480, 4, 4]       [1, 960, 4, 4]       --                   True\n",
              "│    │    └─Sequential (branch_1)        [1, 480, 4, 4]       [1, 512, 4, 4]       247,296              True\n",
              "│    │    └─Sequential (branch_2)        [1, 480, 4, 4]       [1, 256, 4, 4]       268,320              True\n",
              "│    │    └─Sequential (branch_3)        [1, 480, 4, 4]       [1, 128, 4, 4]       252,864              True\n",
              "│    │    └─Sequential (branch_4)        [1, 480, 4, 4]       [1, 64, 4, 4]        30,912               True\n",
              "│    └─MaxPool2d (1)                     [1, 960, 4, 4]       [1, 960, 2, 2]       --                   --\n",
              "├─ResNet_convert_dim (res_3)             [1, 128, 8, 8]       [1, 960, 2, 2]       --                   True\n",
              "│    └─Sequential (block)                [1, 128, 8, 8]       [1, 960, 2, 2]       --                   True\n",
              "│    │    └─Conv2d (0)                   [1, 128, 8, 8]       [1, 960, 2, 2]       123,840              True\n",
              "│    │    └─BatchNorm2d (1)              [1, 960, 2, 2]       [1, 960, 2, 2]       1,920                True\n",
              "│    │    └─ReLU (2)                     [1, 960, 2, 2]       [1, 960, 2, 2]       --                   --\n",
              "├─Sequential (classifier)                [1, 960, 2, 2]       [1, 10]              --                   True\n",
              "│    └─Flatten (0)                       [1, 960, 2, 2]       [1, 3840]            --                   --\n",
              "│    └─Linear (1)                        [1, 3840]            [1, 1024]            3,933,184            True\n",
              "│    └─Dropout (2)                       [1, 1024]            [1, 1024]            --                   --\n",
              "│    └─ReLU (3)                          [1, 1024]            [1, 1024]            --                   --\n",
              "│    └─Linear (4)                        [1, 1024]            [1, 256]             262,400              True\n",
              "│    └─Linear (5)                        [1, 256]             [1, 10]              2,570                True\n",
              "========================================================================================================================\n",
              "Total params: 6,791,540\n",
              "Trainable params: 6,791,540\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 68.95\n",
              "========================================================================================================================\n",
              "Input size (MB): 0.01\n",
              "Forward/backward pass size (MB): 3.18\n",
              "Params size (MB): 22.56\n",
              "Estimated Total Size (MB): 25.74\n",
              "========================================================================================================================"
            ]
          },
          "metadata": {},
          "execution_count": 298
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from pathlib import Path\n",
        "\n",
        "# Download helper functions from Learn PyTorch repo (if not already downloaded)\n",
        "if Path(\"helper_functions.py\").is_file():\n",
        "  print(\"helper_functions.py already exists, skipping download\")\n",
        "else:\n",
        "  print(\"Downloading helper_functions.py\")\n",
        "  # Note: you need the \"raw\" GitHub URL for this to work\n",
        "  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
        "  with open(\"helper_functions.py\", \"wb\") as f:\n",
        "    f.write(request.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtnOuktvn05z",
        "outputId": "9de69e4d-9e75-4d14-c7f3-6415b80452ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "helper_functions.py already exists, skipping download\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(params=network.parameters(),\n",
        "                            lr=0.001,\n",
        "                            momentum=0.9,\n",
        "                            weight_decay=0.01)\n",
        "\n",
        "from helper_functions import accuracy_fn"
      ],
      "metadata": {
        "id": "WKu5E72xn3Fz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(model: torch.nn.Module,\n",
        "              data_loader: torch.utils.data.DataLoader,\n",
        "              loss_fn: torch.nn.Module,\n",
        "              optimizer: torch.optim.Optimizer,\n",
        "              accuracy_fn,\n",
        "              device):\n",
        "\n",
        "    list_train_loss, list_train_acc = [], []\n",
        "    train_loss, train_acc = 0, 0\n",
        "    for batch, (X, y) in enumerate(data_loader):\n",
        "        # Move X and y to the same device as the model\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # 1. forward pass\n",
        "        y_pred = model(X)\n",
        "\n",
        "        # 2. cal loss\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        train_loss += loss\n",
        "        train_acc += accuracy_fn(y_true=y,\n",
        "                                 y_pred=y_pred.argmax(dim=1))\n",
        "\n",
        "        # 3. optimizer 0 grad\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 4. loss back\n",
        "        loss.backward()\n",
        "\n",
        "        # 5. optimizer step\n",
        "        optimizer.step()\n",
        "\n",
        "        if (batch + 1) % 100 == 0:\n",
        "            train_loss_batch = train_loss / (batch + 1)\n",
        "            train_acc_batch = train_acc / (batch + 1)\n",
        "            print(f\"Train batch: {batch + 1} | Train loss: {train_loss_batch:.5f} | Train acc: {train_acc_batch:.5f}\")\n",
        "\n",
        "    train_loss /= len(data_loader)\n",
        "    train_acc /= len(data_loader)\n",
        "\n",
        "    list_train_loss.append(train_loss)\n",
        "    list_train_acc.append(train_acc)\n",
        "\n",
        "    print(f\"Train loss: {train_loss:.5f} | Train acc: {train_acc:.5f}\")\n",
        "    return list_train_loss, list_train_acc"
      ],
      "metadata": {
        "id": "V5qbygYZn5z5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_step(model: torch.nn.Module,\n",
        "             data_loader: torch.utils.data.DataLoader,\n",
        "             loss_fn: torch.nn.Module,\n",
        "             accuracy_fn,\n",
        "             device):\n",
        "\n",
        "    list_test_loss, list_test_acc = [], []\n",
        "    test_loss, test_acc = 0, 0\n",
        "    model.eval()\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        for batch, (X, y) in enumerate(data_loader):\n",
        "            # Move X and y to the same device as the model\n",
        "            X, y = X.to(device), y.to(device)\n",
        "\n",
        "            test_pred = model(X)\n",
        "\n",
        "            test_loss += loss_fn(test_pred, y)\n",
        "            test_acc += accuracy_fn(y_true=y,\n",
        "                                    y_pred=test_pred.argmax(dim=1))\n",
        "\n",
        "            if (batch + 1) % 1000 == 0:\n",
        "                test_loss_batch = test_loss / (batch + 1)\n",
        "                test_acc_batch = test_acc / (batch + 1)\n",
        "                print(f\"Test batch: {batch + 1} | Test loss: {test_loss_batch:.5f} | Test acc: {test_acc_batch:.5f}\")\n",
        "\n",
        "        test_loss /= len(data_loader)\n",
        "        test_acc /= len(data_loader)\n",
        "\n",
        "        list_test_loss.append(test_loss)\n",
        "        list_test_acc.append(test_acc)\n",
        "\n",
        "        print(f\"Test loss: {test_loss:.5f} | Test acc: {test_acc:.5f}\\n\")\n",
        "    return list_test_loss, list_test_acc"
      ],
      "metadata": {
        "id": "drFve8p3n6WT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "epochs = 100\n",
        "\n",
        "train_loss, train_acc = [], []\n",
        "test_loss, test_acc = [], []\n",
        "\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    print(f\"Epoch: {epoch}\")\n",
        "\n",
        "    train_loss, train_acc = train_step(model=network,\n",
        "              data_loader=train_loader,\n",
        "              loss_fn=loss_fn,\n",
        "              optimizer=optimizer,\n",
        "              accuracy_fn=accuracy_fn,\n",
        "              device=device)\n",
        "\n",
        "    test_loss, test_acc = test_step(model=network,\n",
        "             data_loader=test_loader,\n",
        "             loss_fn=loss_fn,\n",
        "             accuracy_fn=accuracy_fn,\n",
        "             device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "af1745a9019c4613ac5f5c9a2f2dcc83",
            "4f780a8242cb48ddb63daf2eb0352aba",
            "79158446b32543d686a8b7249a55e4b8",
            "dce4e628bb5a40b0befabcf201d27ea4",
            "92a632f6ad0f40419120837d94cc762d",
            "11340795b2cf4cfb82548cf02a8951fd",
            "1e53671bff8247e8b224ba0376943a3a",
            "40f893e3d088455fb3e37d5a0cfe70d7",
            "a8b0970397de4d22b198d2ba65aad484",
            "b968c19905eb489a83ca22281ed530c5",
            "e405040a1e73422b9f575a50ec1cf7aa"
          ]
        },
        "id": "J-jDSWR5n83b",
        "outputId": "78b85625-3952-4b4a-ad35-6b615e28097c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "af1745a9019c4613ac5f5c9a2f2dcc83"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\n",
            "Train batch: 100 | Train loss: 2.21355 | Train acc: 16.71875\n",
            "Train batch: 200 | Train loss: 2.10580 | Train acc: 21.28125\n",
            "Train batch: 300 | Train loss: 2.03479 | Train acc: 23.99479\n",
            "Train batch: 400 | Train loss: 1.97596 | Train acc: 26.21094\n",
            "Train batch: 500 | Train loss: 1.93821 | Train acc: 27.84062\n",
            "Train batch: 600 | Train loss: 1.90116 | Train acc: 29.16927\n",
            "Train batch: 700 | Train loss: 1.86993 | Train acc: 30.39286\n",
            "Train loss: 1.84445 | Train acc: 31.48178\n",
            "Test batch: 1000 | Test loss: 2.44434 | Test acc: 17.40000\n",
            "Test batch: 2000 | Test loss: 2.40577 | Test acc: 18.05000\n",
            "Test batch: 3000 | Test loss: 2.39678 | Test acc: 18.70000\n",
            "Test batch: 4000 | Test loss: 2.40898 | Test acc: 18.65000\n",
            "Test batch: 5000 | Test loss: 2.40879 | Test acc: 18.70000\n",
            "Test batch: 6000 | Test loss: 2.40731 | Test acc: 18.56667\n",
            "Test batch: 7000 | Test loss: 2.41018 | Test acc: 18.32857\n",
            "Test batch: 8000 | Test loss: 2.40732 | Test acc: 18.30000\n",
            "Test batch: 9000 | Test loss: 2.41201 | Test acc: 18.16667\n",
            "Test batch: 10000 | Test loss: 2.41268 | Test acc: 18.29000\n",
            "Test loss: 2.41268 | Test acc: 18.29000\n",
            "\n",
            "Epoch: 1\n",
            "Train batch: 100 | Train loss: 1.70665 | Train acc: 37.79688\n",
            "Train batch: 200 | Train loss: 1.66398 | Train acc: 39.29688\n",
            "Train batch: 300 | Train loss: 1.63034 | Train acc: 40.55729\n",
            "Train batch: 400 | Train loss: 1.60892 | Train acc: 41.37109\n",
            "Train batch: 500 | Train loss: 1.59817 | Train acc: 41.82187\n",
            "Train batch: 600 | Train loss: 1.58275 | Train acc: 42.37760\n",
            "Train batch: 700 | Train loss: 1.57561 | Train acc: 42.70312\n",
            "Train loss: 1.56664 | Train acc: 42.94078\n",
            "Test batch: 1000 | Test loss: 1.53887 | Test acc: 43.20000\n",
            "Test batch: 2000 | Test loss: 1.53374 | Test acc: 44.30000\n",
            "Test batch: 3000 | Test loss: 1.53722 | Test acc: 44.03333\n",
            "Test batch: 4000 | Test loss: 1.53972 | Test acc: 44.22500\n",
            "Test batch: 5000 | Test loss: 1.53377 | Test acc: 44.68000\n",
            "Test batch: 6000 | Test loss: 1.53707 | Test acc: 44.76667\n",
            "Test batch: 7000 | Test loss: 1.54502 | Test acc: 44.64286\n",
            "Test batch: 8000 | Test loss: 1.54120 | Test acc: 44.87500\n",
            "Test batch: 9000 | Test loss: 1.54374 | Test acc: 44.73333\n",
            "Test batch: 10000 | Test loss: 1.54158 | Test acc: 44.66000\n",
            "Test loss: 1.54158 | Test acc: 44.66000\n",
            "\n",
            "Epoch: 2\n",
            "Train batch: 100 | Train loss: 1.47112 | Train acc: 46.09375\n",
            "Train batch: 200 | Train loss: 1.45232 | Train acc: 47.11719\n",
            "Train batch: 300 | Train loss: 1.45249 | Train acc: 47.22917\n",
            "Train batch: 400 | Train loss: 1.44674 | Train acc: 47.42969\n",
            "Train batch: 500 | Train loss: 1.43687 | Train acc: 47.81250\n",
            "Train batch: 600 | Train loss: 1.43531 | Train acc: 47.92969\n",
            "Train batch: 700 | Train loss: 1.42684 | Train acc: 48.14732\n",
            "Train loss: 1.42357 | Train acc: 48.29364\n",
            "Test batch: 1000 | Test loss: 1.51612 | Test acc: 44.70000\n",
            "Test batch: 2000 | Test loss: 1.50448 | Test acc: 45.90000\n",
            "Test batch: 3000 | Test loss: 1.50027 | Test acc: 45.43333\n",
            "Test batch: 4000 | Test loss: 1.49450 | Test acc: 46.02500\n",
            "Test batch: 5000 | Test loss: 1.48787 | Test acc: 46.50000\n",
            "Test batch: 6000 | Test loss: 1.48506 | Test acc: 46.85000\n",
            "Test batch: 7000 | Test loss: 1.49921 | Test acc: 46.01429\n",
            "Test batch: 8000 | Test loss: 1.49848 | Test acc: 46.15000\n",
            "Test batch: 9000 | Test loss: 1.50350 | Test acc: 45.94444\n",
            "Test batch: 10000 | Test loss: 1.50133 | Test acc: 45.84000\n",
            "Test loss: 1.50133 | Test acc: 45.84000\n",
            "\n",
            "Epoch: 3\n",
            "Train batch: 100 | Train loss: 1.35856 | Train acc: 50.59375\n",
            "Train batch: 200 | Train loss: 1.37812 | Train acc: 49.85156\n",
            "Train batch: 300 | Train loss: 1.37144 | Train acc: 50.43229\n",
            "Train batch: 400 | Train loss: 1.36703 | Train acc: 50.62500\n",
            "Train batch: 500 | Train loss: 1.35908 | Train acc: 50.94687\n",
            "Train batch: 600 | Train loss: 1.35151 | Train acc: 51.16927\n",
            "Train batch: 700 | Train loss: 1.34950 | Train acc: 51.13839\n",
            "Train loss: 1.34664 | Train acc: 51.19485\n",
            "Test batch: 1000 | Test loss: 1.35679 | Test acc: 51.40000\n",
            "Test batch: 2000 | Test loss: 1.35442 | Test acc: 51.80000\n",
            "Test batch: 3000 | Test loss: 1.35078 | Test acc: 51.83333\n",
            "Test batch: 4000 | Test loss: 1.34891 | Test acc: 51.62500\n",
            "Test batch: 5000 | Test loss: 1.34465 | Test acc: 51.90000\n",
            "Test batch: 6000 | Test loss: 1.34575 | Test acc: 51.66667\n",
            "Test batch: 7000 | Test loss: 1.35674 | Test acc: 51.18571\n",
            "Test batch: 8000 | Test loss: 1.35430 | Test acc: 51.33750\n",
            "Test batch: 9000 | Test loss: 1.35662 | Test acc: 51.06667\n",
            "Test batch: 10000 | Test loss: 1.35434 | Test acc: 51.17000\n",
            "Test loss: 1.35434 | Test acc: 51.17000\n",
            "\n",
            "Epoch: 4\n",
            "Train batch: 100 | Train loss: 1.30589 | Train acc: 52.65625\n",
            "Train batch: 200 | Train loss: 1.31495 | Train acc: 51.97656\n",
            "Train batch: 300 | Train loss: 1.31273 | Train acc: 52.17708\n",
            "Train batch: 400 | Train loss: 1.30431 | Train acc: 52.65625\n",
            "Train batch: 500 | Train loss: 1.30546 | Train acc: 52.77188\n",
            "Train batch: 600 | Train loss: 1.30533 | Train acc: 52.84115\n",
            "Train batch: 700 | Train loss: 1.30211 | Train acc: 52.97545\n",
            "Train loss: 1.30212 | Train acc: 53.00711\n",
            "Test batch: 1000 | Test loss: 1.38378 | Test acc: 48.50000\n",
            "Test batch: 2000 | Test loss: 1.38072 | Test acc: 48.40000\n",
            "Test batch: 3000 | Test loss: 1.36893 | Test acc: 49.50000\n",
            "Test batch: 4000 | Test loss: 1.36647 | Test acc: 49.70000\n",
            "Test batch: 5000 | Test loss: 1.36301 | Test acc: 50.24000\n",
            "Test batch: 6000 | Test loss: 1.36906 | Test acc: 49.80000\n",
            "Test batch: 7000 | Test loss: 1.37814 | Test acc: 49.52857\n",
            "Test batch: 8000 | Test loss: 1.37355 | Test acc: 49.90000\n",
            "Test batch: 9000 | Test loss: 1.37640 | Test acc: 49.67778\n",
            "Test batch: 10000 | Test loss: 1.37332 | Test acc: 49.60000\n",
            "Test loss: 1.37332 | Test acc: 49.60000\n",
            "\n",
            "Epoch: 5\n",
            "Train batch: 100 | Train loss: 1.26798 | Train acc: 54.21875\n",
            "Train batch: 200 | Train loss: 1.27252 | Train acc: 54.21094\n",
            "Train batch: 300 | Train loss: 1.26970 | Train acc: 54.16667\n",
            "Train batch: 400 | Train loss: 1.27499 | Train acc: 53.99219\n",
            "Train batch: 500 | Train loss: 1.27272 | Train acc: 54.07187\n",
            "Train batch: 600 | Train loss: 1.26761 | Train acc: 54.39844\n",
            "Train batch: 700 | Train loss: 1.26509 | Train acc: 54.48438\n",
            "Train loss: 1.26429 | Train acc: 54.50767\n",
            "Test batch: 1000 | Test loss: 1.37428 | Test acc: 49.90000\n",
            "Test batch: 2000 | Test loss: 1.38248 | Test acc: 50.60000\n",
            "Test batch: 3000 | Test loss: 1.36391 | Test acc: 51.33333\n",
            "Test batch: 4000 | Test loss: 1.36140 | Test acc: 51.45000\n",
            "Test batch: 5000 | Test loss: 1.35297 | Test acc: 51.90000\n",
            "Test batch: 6000 | Test loss: 1.35966 | Test acc: 51.55000\n",
            "Test batch: 7000 | Test loss: 1.36671 | Test acc: 51.27143\n",
            "Test batch: 8000 | Test loss: 1.36439 | Test acc: 51.23750\n",
            "Test batch: 9000 | Test loss: 1.36620 | Test acc: 51.02222\n",
            "Test batch: 10000 | Test loss: 1.36527 | Test acc: 51.11000\n",
            "Test loss: 1.36527 | Test acc: 51.11000\n",
            "\n",
            "Epoch: 6\n",
            "Train batch: 100 | Train loss: 1.25425 | Train acc: 54.85938\n",
            "Train batch: 200 | Train loss: 1.24918 | Train acc: 55.10156\n",
            "Train batch: 300 | Train loss: 1.25704 | Train acc: 54.78646\n",
            "Train batch: 400 | Train loss: 1.25160 | Train acc: 54.83594\n",
            "Train batch: 500 | Train loss: 1.24608 | Train acc: 55.15313\n",
            "Train batch: 600 | Train loss: 1.24485 | Train acc: 55.30469\n",
            "Train batch: 700 | Train loss: 1.24574 | Train acc: 55.29688\n",
            "Train loss: 1.24712 | Train acc: 55.22099\n",
            "Test batch: 1000 | Test loss: 1.34162 | Test acc: 50.90000\n",
            "Test batch: 2000 | Test loss: 1.34259 | Test acc: 51.30000\n",
            "Test batch: 3000 | Test loss: 1.32282 | Test acc: 52.70000\n",
            "Test batch: 4000 | Test loss: 1.32137 | Test acc: 52.85000\n",
            "Test batch: 5000 | Test loss: 1.32128 | Test acc: 53.06000\n",
            "Test batch: 6000 | Test loss: 1.32566 | Test acc: 52.73333\n",
            "Test batch: 7000 | Test loss: 1.33205 | Test acc: 52.75714\n",
            "Test batch: 8000 | Test loss: 1.32874 | Test acc: 52.82500\n",
            "Test batch: 9000 | Test loss: 1.33309 | Test acc: 52.60000\n",
            "Test batch: 10000 | Test loss: 1.33081 | Test acc: 52.63000\n",
            "Test loss: 1.33081 | Test acc: 52.63000\n",
            "\n",
            "Epoch: 7\n",
            "Train batch: 100 | Train loss: 1.24925 | Train acc: 55.06250\n",
            "Train batch: 200 | Train loss: 1.25731 | Train acc: 55.00000\n",
            "Train batch: 300 | Train loss: 1.24381 | Train acc: 55.78646\n",
            "Train batch: 400 | Train loss: 1.23574 | Train acc: 55.96875\n",
            "Train batch: 500 | Train loss: 1.23222 | Train acc: 55.93438\n",
            "Train batch: 600 | Train loss: 1.23095 | Train acc: 56.04427\n",
            "Train batch: 700 | Train loss: 1.22809 | Train acc: 56.10268\n",
            "Train loss: 1.22733 | Train acc: 56.07617\n",
            "Test batch: 1000 | Test loss: 1.35991 | Test acc: 51.90000\n",
            "Test batch: 2000 | Test loss: 1.35785 | Test acc: 51.70000\n",
            "Test batch: 3000 | Test loss: 1.34807 | Test acc: 52.10000\n",
            "Test batch: 4000 | Test loss: 1.34401 | Test acc: 52.35000\n",
            "Test batch: 5000 | Test loss: 1.33649 | Test acc: 52.84000\n",
            "Test batch: 6000 | Test loss: 1.34115 | Test acc: 52.75000\n",
            "Test batch: 7000 | Test loss: 1.34452 | Test acc: 52.62857\n",
            "Test batch: 8000 | Test loss: 1.34416 | Test acc: 52.63750\n",
            "Test batch: 9000 | Test loss: 1.34677 | Test acc: 52.42222\n",
            "Test batch: 10000 | Test loss: 1.34507 | Test acc: 52.49000\n",
            "Test loss: 1.34507 | Test acc: 52.49000\n",
            "\n",
            "Epoch: 8\n",
            "Train batch: 100 | Train loss: 1.24081 | Train acc: 56.40625\n",
            "Train batch: 200 | Train loss: 1.23137 | Train acc: 56.27344\n",
            "Train batch: 300 | Train loss: 1.23424 | Train acc: 56.14583\n",
            "Train batch: 400 | Train loss: 1.23116 | Train acc: 56.17578\n",
            "Train batch: 500 | Train loss: 1.23068 | Train acc: 56.23750\n",
            "Train batch: 600 | Train loss: 1.22816 | Train acc: 56.30990\n",
            "Train batch: 700 | Train loss: 1.22550 | Train acc: 56.44420\n",
            "Train loss: 1.22353 | Train acc: 56.49976\n",
            "Test batch: 1000 | Test loss: 1.29976 | Test acc: 54.30000\n",
            "Test batch: 2000 | Test loss: 1.29681 | Test acc: 53.20000\n",
            "Test batch: 3000 | Test loss: 1.28990 | Test acc: 53.73333\n",
            "Test batch: 4000 | Test loss: 1.28838 | Test acc: 53.87500\n",
            "Test batch: 5000 | Test loss: 1.28574 | Test acc: 54.12000\n",
            "Test batch: 6000 | Test loss: 1.28945 | Test acc: 53.90000\n",
            "Test batch: 7000 | Test loss: 1.29817 | Test acc: 53.57143\n",
            "Test batch: 8000 | Test loss: 1.29571 | Test acc: 53.83750\n",
            "Test batch: 9000 | Test loss: 1.29726 | Test acc: 53.71111\n",
            "Test batch: 10000 | Test loss: 1.29576 | Test acc: 53.79000\n",
            "Test loss: 1.29576 | Test acc: 53.79000\n",
            "\n",
            "Epoch: 9\n",
            "Train batch: 100 | Train loss: 1.22234 | Train acc: 55.75000\n",
            "Train batch: 200 | Train loss: 1.21262 | Train acc: 56.46094\n",
            "Train batch: 300 | Train loss: 1.21645 | Train acc: 56.47396\n",
            "Train batch: 400 | Train loss: 1.21484 | Train acc: 56.51172\n",
            "Train batch: 500 | Train loss: 1.20936 | Train acc: 56.60312\n",
            "Train batch: 600 | Train loss: 1.20858 | Train acc: 56.69271\n",
            "Train batch: 700 | Train loss: 1.21015 | Train acc: 56.66071\n",
            "Train loss: 1.20875 | Train acc: 56.77350\n",
            "Test batch: 1000 | Test loss: 1.28787 | Test acc: 53.70000\n",
            "Test batch: 2000 | Test loss: 1.28343 | Test acc: 53.90000\n",
            "Test batch: 3000 | Test loss: 1.27991 | Test acc: 54.20000\n",
            "Test batch: 4000 | Test loss: 1.27610 | Test acc: 54.47500\n",
            "Test batch: 5000 | Test loss: 1.26956 | Test acc: 55.06000\n",
            "Test batch: 6000 | Test loss: 1.27198 | Test acc: 55.01667\n",
            "Test batch: 7000 | Test loss: 1.28079 | Test acc: 54.37143\n",
            "Test batch: 8000 | Test loss: 1.28106 | Test acc: 54.46250\n",
            "Test batch: 9000 | Test loss: 1.28267 | Test acc: 54.33333\n",
            "Test batch: 10000 | Test loss: 1.28095 | Test acc: 54.51000\n",
            "Test loss: 1.28095 | Test acc: 54.51000\n",
            "\n",
            "Epoch: 10\n",
            "Train batch: 100 | Train loss: 1.23084 | Train acc: 55.85938\n",
            "Train batch: 200 | Train loss: 1.22164 | Train acc: 56.33594\n",
            "Train batch: 300 | Train loss: 1.21120 | Train acc: 56.73438\n",
            "Train batch: 400 | Train loss: 1.21063 | Train acc: 56.65625\n",
            "Train batch: 500 | Train loss: 1.20792 | Train acc: 56.67188\n",
            "Train batch: 600 | Train loss: 1.20628 | Train acc: 56.76042\n",
            "Train batch: 700 | Train loss: 1.20953 | Train acc: 56.62277\n",
            "Train loss: 1.20724 | Train acc: 56.80946\n",
            "Test batch: 1000 | Test loss: 1.28201 | Test acc: 55.20000\n",
            "Test batch: 2000 | Test loss: 1.27071 | Test acc: 55.15000\n",
            "Test batch: 3000 | Test loss: 1.26251 | Test acc: 55.20000\n",
            "Test batch: 4000 | Test loss: 1.26768 | Test acc: 54.95000\n",
            "Test batch: 5000 | Test loss: 1.26092 | Test acc: 55.36000\n",
            "Test batch: 6000 | Test loss: 1.26539 | Test acc: 54.85000\n",
            "Test batch: 7000 | Test loss: 1.27288 | Test acc: 54.40000\n",
            "Test batch: 8000 | Test loss: 1.27294 | Test acc: 54.46250\n",
            "Test batch: 9000 | Test loss: 1.27414 | Test acc: 54.30000\n",
            "Test batch: 10000 | Test loss: 1.27246 | Test acc: 54.50000\n",
            "Test loss: 1.27246 | Test acc: 54.50000\n",
            "\n",
            "Epoch: 11\n",
            "Train batch: 100 | Train loss: 1.20296 | Train acc: 56.92188\n",
            "Train batch: 200 | Train loss: 1.20785 | Train acc: 56.75000\n",
            "Train batch: 300 | Train loss: 1.20783 | Train acc: 56.90104\n",
            "Train batch: 400 | Train loss: 1.20847 | Train acc: 56.82031\n",
            "Train batch: 500 | Train loss: 1.21053 | Train acc: 56.66875\n",
            "Train batch: 600 | Train loss: 1.20809 | Train acc: 56.79427\n",
            "Train batch: 700 | Train loss: 1.20914 | Train acc: 56.72545\n",
            "Train loss: 1.21022 | Train acc: 56.78349\n",
            "Test batch: 1000 | Test loss: 1.31318 | Test acc: 54.20000\n",
            "Test batch: 2000 | Test loss: 1.30683 | Test acc: 54.05000\n",
            "Test batch: 3000 | Test loss: 1.29104 | Test acc: 54.96667\n",
            "Test batch: 4000 | Test loss: 1.29626 | Test acc: 54.60000\n",
            "Test batch: 5000 | Test loss: 1.29462 | Test acc: 54.86000\n",
            "Test batch: 6000 | Test loss: 1.29941 | Test acc: 54.11667\n",
            "Test batch: 7000 | Test loss: 1.30481 | Test acc: 53.92857\n",
            "Test batch: 8000 | Test loss: 1.30329 | Test acc: 54.06250\n",
            "Test batch: 9000 | Test loss: 1.30517 | Test acc: 53.84444\n",
            "Test batch: 10000 | Test loss: 1.30442 | Test acc: 54.02000\n",
            "Test loss: 1.30442 | Test acc: 54.02000\n",
            "\n",
            "Epoch: 12\n",
            "Train batch: 100 | Train loss: 1.20765 | Train acc: 57.15625\n",
            "Train batch: 200 | Train loss: 1.19620 | Train acc: 57.64844\n",
            "Train batch: 300 | Train loss: 1.18719 | Train acc: 57.88021\n",
            "Train batch: 400 | Train loss: 1.19197 | Train acc: 57.69141\n",
            "Train batch: 500 | Train loss: 1.19631 | Train acc: 57.51562\n",
            "Train batch: 600 | Train loss: 1.19437 | Train acc: 57.64062\n",
            "Train batch: 700 | Train loss: 1.19780 | Train acc: 57.55134\n",
            "Train loss: 1.20061 | Train acc: 57.45085\n",
            "Test batch: 1000 | Test loss: 1.32590 | Test acc: 53.90000\n",
            "Test batch: 2000 | Test loss: 1.31211 | Test acc: 54.30000\n",
            "Test batch: 3000 | Test loss: 1.30113 | Test acc: 54.43333\n",
            "Test batch: 4000 | Test loss: 1.30846 | Test acc: 53.75000\n",
            "Test batch: 5000 | Test loss: 1.30277 | Test acc: 54.08000\n",
            "Test batch: 6000 | Test loss: 1.30508 | Test acc: 53.78333\n",
            "Test batch: 7000 | Test loss: 1.31330 | Test acc: 53.38571\n",
            "Test batch: 8000 | Test loss: 1.31323 | Test acc: 53.36250\n",
            "Test batch: 9000 | Test loss: 1.31528 | Test acc: 52.91111\n",
            "Test batch: 10000 | Test loss: 1.31472 | Test acc: 52.97000\n",
            "Test loss: 1.31472 | Test acc: 52.97000\n",
            "\n",
            "Epoch: 13\n",
            "Train batch: 100 | Train loss: 1.21323 | Train acc: 56.65625\n",
            "Train batch: 200 | Train loss: 1.20466 | Train acc: 57.03906\n",
            "Train batch: 300 | Train loss: 1.20305 | Train acc: 57.48958\n",
            "Train batch: 400 | Train loss: 1.20072 | Train acc: 57.42969\n",
            "Train batch: 500 | Train loss: 1.19816 | Train acc: 57.48125\n",
            "Train batch: 600 | Train loss: 1.19607 | Train acc: 57.44271\n",
            "Train batch: 700 | Train loss: 1.19914 | Train acc: 57.33259\n",
            "Train loss: 1.19983 | Train acc: 57.24904\n",
            "Test batch: 1000 | Test loss: 1.28266 | Test acc: 53.60000\n",
            "Test batch: 2000 | Test loss: 1.27738 | Test acc: 54.25000\n",
            "Test batch: 3000 | Test loss: 1.26581 | Test acc: 54.80000\n",
            "Test batch: 4000 | Test loss: 1.27190 | Test acc: 54.82500\n",
            "Test batch: 5000 | Test loss: 1.26484 | Test acc: 55.50000\n",
            "Test batch: 6000 | Test loss: 1.26842 | Test acc: 55.41667\n",
            "Test batch: 7000 | Test loss: 1.27295 | Test acc: 55.04286\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-303-d82115e74acf>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m               device=device)\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     test_loss, test_acc = test_step(model=network,\n\u001b[0m\u001b[1;32m     21\u001b[0m              \u001b[0mdata_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m              \u001b[0mloss_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-302-aaf2687274a3>\u001b[0m in \u001b[0;36mtest_step\u001b[0;34m(model, data_loader, loss_fn, accuracy_fn, device)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mtest_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mtest_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-296-5d1056457be1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mx_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mres_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 32 x 16 x 16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mx_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mres_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 128 x 8 x 8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mx_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock_3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 240 x 4 x 4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0mx_4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock_4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mres_3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 960 x 2 x 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m#x_5 = self.block_5(x_4)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-294-1b52ef311fec>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mfirst_block_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbranch_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0msecond_block_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbranch_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mthird_block_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbranch_3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mfourth_block_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbranch_4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mused\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnormalization\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval\u001b[0m \u001b[0mmode\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[0mare\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \"\"\"\n\u001b[0;32m--> 193\u001b[0;31m         return F.batch_norm(\n\u001b[0m\u001b[1;32m    194\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0;31m# If buffers are not to be tracked, ensure that they won't be updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2810\u001b[0m         \u001b[0m_verify_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2812\u001b[0;31m     return torch.batch_norm(\n\u001b[0m\u001b[1;32m   2813\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2814\u001b[0m         \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}